# Elasticsearch Utils - ×¢×¨×›×ª ×›×œ×™× ××§×¦×•×¢×™×ª ×œ-Elasticsearch

×¢×¨×›×ª ×›×œ×™× ××•×ª×××ª ×œ××‘×—× ×™× ×•×¤×¨×•×™×§×˜×™×, ×¢× ×”×¤×¨×“×” ××•×©×œ××ª ×‘×™×Ÿ clients ×•-repositories, ×ª××™×›×” ××œ××” ×‘×¡×™× ×›×¨×•× ×™ ×•××¡×™× ×›×¨×•× ×™.

## ğŸ“ ××‘× ×” ×”×§×‘×¦×™× - CLEAN ARCHITECTURE

```
shared-utilities/elasticsearch/
â”œâ”€â”€ __init__.py                      # ×™×‘×•××™× ×•×¤×•× ×§×¦×™×•×ª ××”×™×¨×•×ª
â”œâ”€â”€ elasticsearch_sync_client.py     # ×œ×§×•×— ×¡×™× ×›×¨×•× ×™ (CONNECTION ONLY)
â”œâ”€â”€ elasticsearch_async_client.py    # ×œ×§×•×— ××¡×™× ×›×¨×•× ×™ (CONNECTION ONLY)  
â”œâ”€â”€ elasticsearch_sync_repository.py # repository ×¡×™× ×›×¨×•× ×™ (CRUD ONLY)
â”œâ”€â”€ elasticsearch_async_repository.py# repository ××¡×™× ×›×¨×•× ×™ (CRUD ONLY)
â”œâ”€â”€ examples.py                      # ×“×•×’×××•×ª ××§×™×¤×•×ª
â””â”€â”€ README.md                        # ×”××“×¨×™×š ×”×–×”
```

## ğŸš€ ×”×ª×§× ×” ××”×™×¨×”

```bash
pip install elasticsearch
pip install pandas  # ×œ××™× ×˜×’×¨×¦×™×™×ª DataFrame
```

## ğŸ¯ ×”×¢×§×¨×•× ×•×ª ×”×—×“×©×™× - FIXED ARCHITECTURE

### âœ… **×”×”×¤×¨×“×” ×”××•×©×œ××ª:**
- **Connection Clients:** ×¨×§ ×—×™×‘×•×¨ ×•×¤×¢×•×œ×•×ª RAW (`execute_*`)
- **CRUD Repositories:** ×¨×§ ×œ×•×’×™×§×” ×¢×¡×§×™×ª (`create`, `search`, `aggregate`)
- **4 ×§×œ××¡×™× × ×¤×¨×“×™×:** ×œ× ×¢×•×“ ×‘×œ×’×Ÿ ×©×œ sync/async ×‘××•×ª×• ××§×•×!
- **××™×Ÿ ×¢×•×“ runtime detection ××›×•×¢×¨!**

### âŒ **××” ×ª×™×§× ×•:**
- âŒ ××™×Ÿ ×™×•×ª×¨ Repository ×™×—×™×“ ×¢× `if self.is_async`
- âŒ ××™×Ÿ ×™×•×ª×¨ `RuntimeError` exceptions
- âŒ ××™×Ÿ ×™×•×ª×¨ ×‘×œ×’×Ÿ ×©×œ aggregations ×‘client

---

## ğŸ“Š ××” ×–×” Elasticsearch? - ×œ××‘×—×Ÿ

### **Elasticsearch vs SQL:**
| SQL Database | â†’ | Elasticsearch |
|-------------|---|---------------|
| Database    | â†’ | Index         |
| Table       | â†’ | Index         |
| Row         | â†’ | Document      |
| Column      | â†’ | Field         |
| Schema      | â†’ | Mapping       |
| WHERE       | â†’ | Query DSL     |

### **××•×©×’×™ ×™×¡×•×“ ×œ××‘×—×Ÿ:**
- **Index** - ××›×œ documents ×××•×ª×• ×¡×•×’ (×›××• table)
- **Document** - ×¨×©×•××” ×‘×¤×•×¨××˜ JSON ×¢× `_id` ×™×™×—×•×“×™
- **Mapping** - ×”×’×“×¨×ª ×¡×•×’ ×”×©×“×•×ª (`text`, `keyword`, `date`, `float`)
- **Query DSL** - ×©×¤×ª ×—×™×¤×•×© ×’××™×©×” ×©×œ Elasticsearch
- **Aggregations** - × ×™×ª×•×—×™× ×•×¡×˜×˜×™×¡×˜×™×§×•×ª (×›××• GROUP BY)
- **Shards** - ×—×œ×•×§×” ×¤× ×™××™×ª ×œ×‘×™×¦×•×¢×™×
- **Replicas** - ×¢×•×ª×§×™× ×œ×–××™× ×•×ª ×’×‘×•×”×”

---

## ğŸ”§ ×©×™××•×© ×‘×¡×™×¡×™ - Sync

### 1. ×”×§××” ××”×™×¨×”
```python
from shared_utilities.elasticsearch import ElasticsearchSyncClient, ElasticsearchSyncRepository

# ×™×¦×™×¨×ª client (CONNECTION ONLY) ×•-repository (CRUD ONLY)
client = ElasticsearchSyncClient("http://localhost:9200")
repo = ElasticsearchSyncRepository(client, "products")

# ×”×§××ª index ×¢× mapping
mapping = {
    "properties": {
        "name": {"type": "text", "analyzer": "standard"},
        "price": {"type": "float"},
        "category": {"type": "keyword"},  # ×œ×¡×™× ×•×Ÿ ××“×•×™×§
        "description": {"type": "text"},  # ×œ×—×™×¤×•×© ×˜×§×¡×˜
        "in_stock": {"type": "boolean"},
        "created_at": {"type": "date"},
        "tags": {"type": "keyword"}       # ××¢×¨×š ×©×œ ××™×œ×•×ª ××¤×ª×—
    }
}

settings = {
    "number_of_shards": 1,
    "number_of_replicas": 0  # ×œ×¤×™×ª×•×— ××§×•××™
}

repo.initialize_index(mapping, settings)
```

### 2. CRUD ×‘×¡×™×¡×™
```python
# CREATE - ×™×¦×™×¨×ª document
doc_id = repo.create({
    "name": "Gaming Laptop",
    "price": 1299.99,
    "category": "electronics",
    "description": "High-performance gaming laptop with RTX graphics",
    "in_stock": True,
    "tags": ["gaming", "laptop", "rtx"]
})
print(f"Created document: {doc_id}")

# READ - ×§×¨×™××ª document
product = repo.get_by_id(doc_id)
print(f"Product: {product['name']} - ${product['price']}")

# UPDATE - ×¢×“×›×•×Ÿ document  
success = repo.update(doc_id, {
    "price": 1199.99,
    "in_stock": False,
    "tags": ["gaming", "laptop", "rtx", "sale"]
})

# DELETE - ××—×™×§×ª document
success = repo.delete(doc_id)
```

### 3. ×—×™×¤×•×© ××ª×§×“× - Query Building System
```python
# ×—×™×¤×•×© ×˜×§×¡×˜ ×—×•×¤×©×™ (full-text search)
results = repo.search(
    text_search="gaming laptop",
    limit=10
)
print(f"Found {results['total_hits']} results")

# ×—×™×¤×•×© ××“×•×™×§ (exact match) - term filters
electronics = repo.search(
    term_filters={"category": "electronics", "in_stock": True},
    limit=20
)

# ×—×™×¤×•×© ×˜×•×•×— (range filters)
expensive_products = repo.search(
    range_filters={"price": {"gte": 1000, "lte": 5000}},
    limit=15
)

# ×—×™×¤×•×© ××©×•×œ×‘ - ×›×œ ×”×¤×™×œ×˜×¨×™× ×‘×™×—×“
filtered_results = repo.search(
    text_search="laptop",                           # ×—×™×¤×•×© ×˜×§×¡×˜
    term_filters={"category": "electronics", "in_stock": True},  # ××“×•×™×§
    range_filters={"price": {"lte": 2000}},        # ×˜×•×•×—
    sort_by=[{"price": {"order": "asc"}}],         # ××™×•×Ÿ
    limit=10,
    offset=0
)

# ×—×™×¤×•×© wildcard (×ª×•×•×™× ×›×œ×œ×™×™×)
wireless_products = repo.search(
    wildcard_filters={"name": "*wireless*"},
    limit=10
)

# ×—×™×¤×•×© fuzzy (×¡×œ×—× ×™ ×œ×©×’×™××•×ª ×›×ª×™×‘)
fuzzy_results = repo.search(
    fuzzy_search={"name": "lapto"},  # ×™×—×¤×© "laptop"
    limit=5
)

# ×‘×“×™×§×ª ×§×™×•× ×©×“×•×ª
has_tags = repo.search(
    exists_filters=["tags", "description"],
    limit=10
)
```

### 4. Bulk Operations - ×¢×™×‘×•×“ ××¡×™×‘×™
```python
# Bulk create - ×™×¦×™×¨×” ××¡×™×‘×™×ª (×”×›×™ ×™×¢×™×œ!)
products = [
    {"name": "Wireless Mouse", "price": 29.99, "category": "electronics"},
    {"name": "Mechanical Keyboard", "price": 79.99, "category": "electronics"},
    {"name": "4K Monitor", "price": 299.99, "category": "electronics"},
    {"name": "Python Guide", "price": 39.99, "category": "books"}
]
result = repo.bulk_create(products)
print(f"Created: {result['success_count']}/{len(products)} products")

# Bulk update - ×¢×“×›×•×Ÿ ××¡×™×‘×™
updates = [
    {"_id": "product1", "doc": {"price": 25.99, "in_stock": False}},
    {"_id": "product2", "doc": {"price": 75.99, "tags": ["keyboard", "sale"]}}
]
result = repo.bulk_update(updates)
print(f"Updated: {result['success_count']} products")

# Bulk delete - ××—×™×§×” ××¡×™×‘×™×ª
result = repo.bulk_delete(["product1", "product2", "product3"])
print(f"Deleted: {result['success_count']} products")
```

---

## âš¡ ×©×™××•×© ××¡×™× ×›×¨×•× ×™ - Async

### ×”×’×“×¨×” ××¡×™× ×›×¨×•× ×™×ª
```python
import asyncio
from shared_utilities.elasticsearch import ElasticsearchAsyncClient, ElasticsearchAsyncRepository

async def async_example():
    # ×™×¦×™×¨×ª client ×•-repository ××¡×™× ×›×¨×•× ×™×™×
    client = ElasticsearchAsyncClient("http://localhost:9200")
    await client.connect()  # âš ï¸ ×—×©×•×‘!
    repo = ElasticsearchAsyncRepository(client, "async_products")
    
    # ×›×œ ×”×¤×¢×•×œ×•×ª ×–×”×•×ª ××‘×œ ×¢× await
    doc_id = await repo.create({"name": "Async Product", "price": 99.99})
    product = await repo.get_by_id(doc_id)
    
    results = await repo.search(
        text_search="async",
        term_filters={"price": {"gte": 50}},
        limit=10
    )
    
    bulk_result = await repo.bulk_create([
        {"name": "Product 1", "price": 10.99},
        {"name": "Product 2", "price": 20.99}
    ])
    
    # âš ï¸ ×—×©×•×‘ ×œ×¡×’×•×¨!
    await client.close()
    
    return results

# ×”×¨×¦×”
results = asyncio.run(async_example())
```

---

## ğŸ“ˆ Aggregations - × ×™×ª×•×—×™× ×•×¡×˜×˜×™×¡×˜×™×§×•×ª

### ××’×¨×’×¦×™×•×ª × ×¤×•×¦×•×ª ×œ××‘×—×Ÿ
```python
# 1. ×××•×¦×¢ ××—×™×¨×™× ×œ×¤×™ ×§×˜×’×•×¨×™×” (×›××• GROUP BY + AVG)
agg_results = repo.aggregate({
    "avg_price_by_category": {
        "terms": {"field": "category"},  # ×§×™×‘×•×¥ ×œ×¤×™ ×§×˜×’×•×¨×™×”
        "aggs": {
            "avg_price": {"avg": {"field": "price"}}  # ×××•×¦×¢ ××—×™×¨
        }
    }
})

for bucket in agg_results['avg_price_by_category']['buckets']:
    category = bucket['key']
    count = bucket['doc_count']
    avg_price = bucket['avg_price']['value']
    print(f"{category}: {count} products, avg price: ${avg_price:.2f}")

# 2. ×¡×˜×˜×™×¡×˜×™×§×•×ª ×‘×¡×™×¡×™×•×ª (min, max, avg, sum, count)
stats = repo.aggregate({
    "price_stats": {"stats": {"field": "price"}}
})
print(f"Price stats: min=${stats['price_stats']['min']}, max=${stats['price_stats']['max']}")

# 3. ×”×ª×¤×œ×’×•×ª ×˜×•×•×—×™ ××—×™×¨×™×
price_ranges = repo.aggregate({
    "price_ranges": {
        "range": {
            "field": "price",
            "ranges": [
                {"to": 50, "key": "cheap"},
                {"from": 50, "to": 200, "key": "medium"},
                {"from": 200, "key": "expensive"}
            ]
        }
    }
})

for bucket in price_ranges['price_ranges']['buckets']:
    range_key = bucket['key']
    count = bucket['doc_count']
    print(f"{range_key}: {count} products")

# 4. ××’×¨×’×¦×™×” ××•×ª× ×™×ª (×¢× ×¤×™×œ×˜×¨×™×)
filtered_agg = repo.aggregate(
    {"avg_electronics_price": {"avg": {"field": "price"}}},
    term_filters={"category": "electronics", "in_stock": True}
)

# 5. ×¡×¤×™×¨×” ×¤×©×•×˜×”
electronics_count = repo.count(term_filters={"category": "electronics"})
expensive_count = repo.count(range_filters={"price": {"gte": 500}})
```

---

## ğŸ¼ ××™× ×˜×’×¨×¦×™×” ×¢× Pandas

### DataFrame â†” Elasticsearch
```python
import pandas as pd

# 1. ×™×¦×™×¨×ª × ×ª×•× ×™× ×‘-DataFrame
df = pd.DataFrame({
    'product_id': ['P001', 'P002', 'P003'],
    'name': ['Gaming Mouse', 'Keyboard', 'Monitor'],
    'price': [49.99, 79.99, 299.99],
    'category': ['electronics', 'electronics', 'electronics'],
    'in_stock': [True, False, True]
})

# 2. ×”×¢×œ××” ×-DataFrame ×œ-Elasticsearch
result = repo.bulk_create_from_dataframe(df, id_column='product_id')
print(f"Uploaded: {result['success_count']} records")

# 3. ×”×•×¨×“×” ×-Elasticsearch ×œ-DataFrame
search_df = repo.to_dataframe(
    term_filters={"category": "electronics"},
    range_filters={"price": {"gte": 50}},
    limit=100
)

# 4. × ×™×ª×•×— ×¢× pandas
if not search_df.empty:
    total_revenue = search_df['price'].sum()
    avg_price = search_df['price'].mean()
    category_counts = search_df['category'].value_counts()
    
    print(f"Total value: ${total_revenue:.2f}")
    print(f"Average price: ${avg_price:.2f}")
    print("Category breakdown:")
    print(category_counts)
```

---

## ğŸ” Scroll Search - ×¢×™×‘×•×“ × ×ª×•× ×™× ×’×“×•×œ×™×

### ×¡×¨×™×§×ª ×›××•×™×•×ª ×’×“×•×œ×•×ª (×œ× deep pagination!)
```python
# 1. ×¡×¨×™×§×” ×“×¨×š ×›×œ ×”documents
all_products = []
for product in repo.scroll_search(scroll_size=1000):  # ×‘×¦×‘×™ ×©×œ 1000
    all_products.append(product)
    if len(all_products) >= 10000:  # ×”×’×‘×œ×” ×œ×“×•×’××”
        break

print(f"Processed {len(all_products)} products")

# 2. ×¡×¨×™×§×” ×¢× ×¤×™×œ×˜×¨×™×
expensive_products = []
for product in repo.scroll_search(
    range_filters={"price": {"gte": 500}},
    scroll_size=500
):
    expensive_products.append(product)

# 3. ×¢×™×‘×•×“ streaming (×—×¡×›×•× ×™ ×‘×–×™×›×¨×•×Ÿ)
def process_products_streaming():
    total_value = 0
    count = 0
    
    for product in repo.scroll_search(scroll_size=1000):
        total_value += product.get('price', 0)
        count += 1
        
        if count % 1000 == 0:
            print(f"Processed {count} products, total value: ${total_value:.2f}")
    
    return total_value, count

total, count = process_products_streaming()
```

---

## ğŸ¯ ×“×¤×•×¡×™× × ×¤×•×¦×™× ×œ××‘×—×Ÿ

### 1. ×—×™×¤×•×© ××•×¨×›×‘
```python
# ×©××œ×ª ××‘×—×Ÿ ×˜×™×¤×•×¡×™×ª: "××¦× ××•×¦×¨×™ ××œ×§×˜×¨×•× ×™×§×” ×‘××œ××™, ×‘××—×™×¨ 100-500$, ×©××›×™×œ×™× 'gaming'"
results = repo.search(
    text_search="gaming",
    term_filters={"category": "electronics", "in_stock": True},
    range_filters={"price": {"gte": 100, "lte": 500}},
    sort_by=[{"price": {"order": "asc"}}],
    limit=20
)
```

### 2. × ×™×ª×•×— × ×ª×•× ×™×
```python
# ×©××œ×ª ××‘×—×Ÿ: "×—×©×‘ ×××•×¦×¢ ××—×™×¨ ×œ×¤×™ ×§×˜×’×•×¨×™×” ×•××¦× ×”×§×˜×’×•×¨×™×” ×”×™×§×¨×” ×‘×™×•×ª×¨"
agg_results = repo.aggregate({
    "categories": {
        "terms": {"field": "category"},
        "aggs": {"avg_price": {"avg": {"field": "price"}}}
    }
})

# ×¢×™×‘×•×“ ×”×ª×•×¦××•×ª
category_prices = {}
for bucket in agg_results['categories']['buckets']:
    category = bucket['key']
    avg_price = bucket['avg_price']['value']
    category_prices[category] = avg_price

most_expensive_category = max(category_prices, key=category_prices.get)
print(f"Most expensive category: {most_expensive_category}")
```

### 3. Bulk Operations
```python
# ×©××œ×ª ××‘×—×Ÿ: "×”×¢×œ×” 1000 ××•×¦×¨×™× ××§×•×‘×¥ CSV"
import pandas as pd

# ×§×¨×™××ª CSV
df = pd.read_csv("products.csv")

# × ×™×§×•×™ × ×ª×•× ×™×
df = df.dropna()
df['price'] = pd.to_numeric(df['price'], errors='coerce')

# ×”×¢×œ××” ××¡×™×‘×™×ª
result = repo.bulk_create_from_dataframe(df, id_column='product_id')
print(f"Success: {result['success_count']}, Errors: {result['error_count']}")
```

### 4. ××¢×§×‘ ××—×¨ ×©×™× ×•×™×™×
```python
# ×¢×“×›×•×Ÿ ××—×™×¨×™× ×•×”×•×¡×¤×ª timestamp
updates = []
for product_id in product_ids:
    updates.append({
        "_id": product_id,
        "doc": {
            "price": new_price,
            "last_updated": datetime.now(),
            "price_changed": True
        }
    })

result = repo.bulk_update(updates)
```

---

## âš¡ ×¤×•× ×§×¦×™×•×ª ××”×™×¨×•×ª ×œ××‘×—×Ÿ

### Setup ××”×™×¨
```python
# Sync
from shared_utilities.elasticsearch import setup_elasticsearch_sync
client, repo = setup_elasticsearch_sync("http://localhost:9200", "products")

# Async
from shared_utilities.elasticsearch import setup_elasticsearch_async
client, repo = await setup_elasticsearch_async("http://localhost:9200", "products")
```

### ×¤×§×•×“×•×ª ×—×™×•× ×™×•×ª
```python
# ×™×¦×™×¨×” ××”×™×¨×”
doc_id = repo.create({"name": "Test", "price": 99.99})

# ×—×™×¤×•×© ××”×™×¨
results = repo.search(text_search="test", limit=5)

# ×¡×¤×™×¨×” ××”×™×¨×”  
count = repo.count(term_filters={"category": "electronics"})

# ××’×¨×’×¦×™×” ××”×™×¨×”
stats = repo.aggregate({"avg_price": {"avg": {"field": "price"}}})

# DataFrame ××”×™×¨
df = repo.to_dataframe(limit=100)
```

---

## ğŸš¨ ×©×’×™××•×ª × ×¤×•×¦×•×ª ×•×¤×ª×¨×•× ×•×ª

### 1. ×‘×¢×™×•×ª ×—×™×‘×•×¨
```python
# Elasticsearch ×œ× ×¨×¥
# ×¤×ª×¨×•×Ÿ: docker run -p 9200:9200 -e "discovery.type=single-node" elasticsearch:7.17.0
```

### 2. Index ×œ× ×§×™×™×
```python
# ×‘×“×™×§×” ×× index ×§×™×™×
if not repo.index_exists():
    repo.initialize_index(mapping)
```

### 3. Mapping conflicts
```python
# ××—×™×§×” ×•×™×¦×™×¨×” ××—×“×©
repo.delete_index()
repo.initialize_index(new_mapping)
```

### 4. ×©×’×™××•×ª bulk
```python
result = repo.bulk_create(documents)
if result['error_count'] > 0:
    print(f"Had {result['error_count']} errors")
```

---

## ğŸ“ ×˜×™×¤×™× ×œ××‘×—×Ÿ

### ××” ×—×©×•×‘ ×œ×“×¢×ª:
1. **Query Types:** match (×˜×§×¡×˜), term (××“×•×™×§), range (×˜×•×•×—), wildcard, fuzzy
2. **Mapping Types:** text (×—×™×¤×•×©), keyword (×¡×™× ×•×Ÿ), date, float, boolean
3. **Aggregations:** terms (×§×™×‘×•×¥), avg/sum/min/max (×—×™×©×•×‘×™×), range (×˜×•×•×—×™×)
4. **Bulk Operations:** ×ª××™×“ ×™×•×ª×¨ ×™×¢×™×œ ××¤×¢×•×œ×•×ª ×™×—×™×“×•×ª
5. **Scroll vs Pagination:** scroll ×œ× ×ª×•× ×™× ×’×“×•×œ×™×, pagination ×œ×××©×§

### ×“×¤×•×¡×™ ×§×•×“ ×©×—×•×–×¨×™× ×‘××‘×—× ×™×:
```python
# 1. ×™×¦×™×¨×ª index ×¢× mapping
repo.initialize_index(mapping)

# 2. ×—×™×¤×•×© ××©×•×œ×‘
results = repo.search(
    text_search="query",
    term_filters={"field": "value"},
    range_filters={"price": {"gte": 100}}
)

# 3. ××’×¨×’×¦×™×” ×‘×¡×™×¡×™×ª
agg = repo.aggregate({"group": {"terms": {"field": "category"}}})

# 4. DataFrame integration
df = repo.to_dataframe()
repo.bulk_create_from_dataframe(df)
```

**×‘×”×¦×œ×—×” ×‘××‘×—×Ÿ! ğŸ¯**

---

## ğŸ”§ Troubleshooting ××”×™×¨

| ×‘×¢×™×” | ×¤×ª×¨×•×Ÿ |
|------|--------|
| Connection refused | `docker run -p 9200:9200 elasticsearch:7.17.0` |
| Index not found | `repo.initialize_index(mapping)` |
| Mapping conflict | `repo.delete_index()` + create again |
| Slow queries | ×”×©×ª××© ×‘-term ×‘××§×•× match ×œ×¡×™× ×•×Ÿ |
| Memory issues | ×”×©×ª××© ×‘-scroll ×‘××§×•× pagination ×’×“×•×œ |

**×–×” ×”×›×œ! Elasticsearch ××•×›×Ÿ ×œ××‘×—×Ÿ! ğŸš€**