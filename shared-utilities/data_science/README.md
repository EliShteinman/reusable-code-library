# Data Science Utilities - ×¢×¨×›×ª ×›×œ×™× ××ª×§×“××ª ×œ× ×™×ª×•×— × ×ª×•× ×™×

×¢×¨×›×ª ×›×œ×™× ××§×¦×•×¢×™×ª ×¢× **××¨×›×™×˜×§×˜×•×¨×” ×›×¤×•×œ×”**: ×™×™×©×•× ×‘×¡×™×¡×™ ×©×ª××™×“ ×¢×•×‘×“ + ×™×™×©×•× ×’× ×¨×™ ××ª×§×“× ×¢× ×ª××™×›×” ×‘×¡×¤×¨×™×•×ª ××¨×•×‘×•×ª.

## ğŸš€ ×”×ª×§× ×” ××”×™×¨×”

```bash
pip install pandas numpy nltk
pip install textblob spacy openpyxl  # ××•×¤×¦×™×•× ×œ×™ ×œ×××¤×™×™× ×™× ××ª×§×“××™×
```

## ğŸ“– ×©×™××•×© ×‘×¡×™×¡×™ (×œ×ª×—×™×œ×ª ××‘×—×Ÿ)

```python
from shared_utilities.data_science import *

# ×˜×¢×™× ×ª × ×ª×•× ×™×
loader = UniversalDataLoader()
df = loader.load_data("data.csv")

# × ×™×§×•×™ ×˜×§×¡×˜
cleaner = TextCleaner()
df_clean = cleaner.clean_dataframe(df, ['text_column'])

# × ×™×ª×•×— ×¨×’×©×•×ª
sentiment_analyzer = SentimentAnalyzer()
df_with_sentiment = sentiment_analyzer.analyze_dataframe(df_clean, 'text_column')

# × ×™×ª×•×— ×˜×§×¡×˜
analyzer = TextAnalyzer()
report = analyzer.generate_summary_report(df_with_sentiment, 'text_column')
```

## âš¡ ×©×™××•×© ××ª×§×“× (×œ×¦×™×•× ×™× ×’×‘×•×”×™×)

### ğŸ”§ Factory Pattern ×œ××‘×—× ×™×

```python
# ×™×¦×™×¨×ª analyzer ×¡×¤×¦×™×¤×™ ×œ×¤×™ ×“×¨×™×©×ª ×”××‘×—×Ÿ
sentiment_analyzer = ProcessorFactory.create_sentiment_analyzer("textblob")
text_processor = ProcessorFactory.create_text_processor("spacy")

# ×× ×”×¡×¤×¨×™×™×” ×œ× ×–××™× ×” - fallback ××•×˜×•××˜×™
smart_analyzer = SmartSentimentAnalyzer()  # ×‘×•×—×¨ ×”×˜×•×‘ ×‘×™×•×ª×¨
```

### ğŸŒŸ ××•×œ×˜×™-×¡×¤×¨×™×•×ª (×¢××™×“×•×ª ×‘××‘×—×Ÿ)

```python
# ×‘×“×™×§×” ××” ×–××™×Ÿ
available_analyzers = get_available_sentiment_analyzers()
print(f"Sentiment analyzers: {available_analyzers}")

# ×©×™××•×© ×‘-ensemble ×œ×“×™×•×§ ×’×‘×•×”
ensemble = EnsembleSentimentAnalyzer(['vader', 'textblob', 'fallback'])
result = ensemble.analyze_sentiment("Amazing product!")
```

### ğŸ‡®ğŸ‡± ×ª××™×›×” ×‘×¢×‘×¨×™×ª

```python
# ×¢×™×‘×•×“ ×˜×§×¡×˜ ×¢×‘×¨×™ ××ª×§×“×
hebrew_processor = HebrewTextProcessor()
roots = hebrew_processor.extract_roots("×”××•×¦×¨ ×”×–×” ××¢×•×œ×” ×•××•××œ×¥")
stems = hebrew_processor.extract_stems("×× ×™ ××•×”×‘ ××ª ×”×˜×›× ×•×œ×•×’×™×” ×”×—×“×©×”")

# × ×™×ª×•×— ×¨×’×©×•×ª ×¢×‘×¨×™
hebrew_sentiment = HebrewSentimentAnalyzer()
result = hebrew_sentiment.analyze_sentiment("×–×” ××•×¦×¨ ××“×”×™×!")
```

### ğŸ¯ NLP ××ª×§×“× (Stemming & Lemmatization)

```python
# NLTK ××ª×§×“×
nltk_processor = NLTKTextProcessor()
stems = nltk_processor.extract_stems("running dogs are eating quickly")
lemmas = nltk_processor.extract_lemmas("running dogs are eating quickly")

# spaCy ××ª×§×“×
spacy_processor = SpaCyTextProcessor()
lemmas = spacy_processor.extract_lemmas("The running dogs are eating")
```

## ğŸ§ª ×¤×•× ×§×¦×™×•×ª ××”×™×¨×•×ª ×œ××‘×—×Ÿ

### Pipeline ××œ×

```python
# × ×™×ª×•×— ××œ× ×‘×©×•×¨×” ××—×ª
results = quick_text_analysis_pipeline(
    file_path="reviews.csv",
    text_column="review_text",
    category_column="product_type"
)

print(f"Processed {results['processing_info']['final_rows']} texts")
print(f"Sentiment distribution: {results['analysis_report']['common_words']}")
```

### ×‘×“×™×§×ª ×¨×’×©×•×ª ××”×™×¨×”

```python
# ×‘×“×™×§×” ××”×™×¨×” ×œ×˜×§×¡×˜ ×‘×•×“×“
result = quick_sentiment_check("I love this product!", analyzer="vader")
print(f"Sentiment: {result['label']} (score: {result['compound']})")

# ×¢× fallback ××•×˜×•××˜×™ ×× VADER ×œ× ×–××™×Ÿ
result = quick_sentiment_check("Great experience!", analyzer="auto")
```

### ×—×™×œ×•×¥ ×××¤×™×™× ×™× ××œ×

```python
# ×›×œ ×”×××¤×™×™× ×™× ×‘×¤×¢×•×œ×” ××—×ª
features = extract_all_features("The running dogs are eating delicious food")
print(f"Tokens: {features['tokens']}")
print(f"Stems: {features['stems']}")
print(f"Lemmas: {features['lemmas']}")
print(f"Sentiment: {features['sentiment']}")
```

## ğŸ“ ×ª×¨×—×™×©×™ ××‘×—×Ÿ × ×¤×•×¦×™×

### ×ª×¨×—×™×© 1: "×”×©×ª××© ×‘-TextBlob"
```python
# ×”××‘×—×Ÿ ×“×•×¨×© TextBlob ×¡×¤×¦×™×¤×™
try:
    analyzer = ProcessorFactory.create_sentiment_analyzer("textblob")
    result = analyzer.analyze_sentiment(text)
except:
    # Fallback ××•×˜×•××˜×™
    analyzer = SmartSentimentAnalyzer()
    result = analyzer.analyze_sentiment(text)
```

### ×ª×¨×—×™×© 2: "×‘×¦×¢ stemming ×•-lemmatization"
```python
# ×¢× NLTK
processor = NLTKTextProcessor()
stems = processor.extract_stems(text)
lemmas = processor.extract_lemmas(text)

# ×¢× spaCy (×× ×–××™×Ÿ)
spacy_processor = SpaCyTextProcessor()
lemmas = spacy_processor.extract_lemmas(text)
```

### ×ª×¨×—×™×© 3: "× ×™×ª×•×— ×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª"
```python
# ××™×•×—×“ ×œ×¢×‘×¨×™×ª
hebrew_processor = HebrewTextProcessor()
tokens = hebrew_processor.tokenize("×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª")
roots = hebrew_processor.extract_roots("×”××™×œ×™× ×”××œ×”")
sentiment = HebrewSentimentAnalyzer().analyze_sentiment("××•×¦×¨ ××¢×•×œ×”!")
```

### ×ª×¨×—×™×© 4: Pipeline ×‘×˜×•×— ×œ××‘×—×Ÿ
```python
# ×™×¦×™×¨×ª pipeline ×©×ª××™×“ ×™×¢×‘×•×“
config = create_exam_safe_pipeline(preferred_sentiment="vader")
pipeline_result = quick_text_analysis_pipeline(
    file_path="data.csv",
    text_column="text"
)
```

## ğŸ“ ××‘× ×” ×”×¡×¤×¨×™×™×”

```
shared-utilities/data_science/
â”œâ”€â”€ __init__.py                          # Import ×›×œ ×”×¤×•× ×§×¦×™×•×ª
â”‚
â”œâ”€â”€ data_loader.py                       # ×˜×¢×™× ×ª ×§×‘×¦×™× (CSV, JSON, Excel)
â”œâ”€â”€ text_cleaner.py                      # × ×™×§×•×™ ×˜×§×¡×˜ ×‘×¡×™×¡×™
â”œâ”€â”€ text_analyzer.py                     # × ×™×ª×•×— ×˜×§×¡×˜ ×¡×˜×˜×™×¡×˜×™
â”œâ”€â”€ sentiment_analyzer.py                # × ×™×ª×•×— ×¨×’×©×•×ª ×‘×¡×™×¡×™
â”‚
â”œâ”€â”€ text_processing_base.py              # ABC Classes + Factory
â”œâ”€â”€ sentiment_implementations.py         # ×›×œ ×”-sentiment analyzers
â”œâ”€â”€ text_processing_implementations.py   # Stemming, Lemmatization, Hebrew
â”‚
â”œâ”€â”€ examples.py                          # ×“×•×’×××•×ª ×‘×¡×™×¡×™×•×ª
â”œâ”€â”€ examples_advanced.py                 # ×“×•×’×××•×ª ××ª×§×“××•×ª
â””â”€â”€ README.md                           # ×”××“×¨×™×š ×”×–×”
```

## ğŸ”§ Troubleshooting ×œ××‘×—×Ÿ

### ×‘×¢×™×”: NLTK ×œ× × ××¦×
```python
# ×”×¡×¤×¨×™×™×” ×ª×¢×‘×•×¨ ××•×˜×•××˜×™×ª ×œ-fallback
# ××• ×”×ª×§×Ÿ: pip install nltk
```

### ×‘×¢×™×”: spaCy ×œ× × ××¦×
```python
# pip install spacy
# python -m spacy download en_core_web_sm
```

### ×‘×¢×™×”: ×§×•×‘×¥ ×œ× × ×˜×¢×Ÿ
```python
# ×‘×“×•×§ ×¤×•×¨××˜×™× × ×ª××›×™×
loader = UniversalDataLoader()
info = loader.get_file_info("file.csv")
print(info)
```

## ğŸ¯ ×˜×™×¤×™× ×œ××‘×—×Ÿ

### 1. **×ª××™×“ ×”×ª×—×œ ×‘×¡×™×¡×™:**
```python
from shared_utilities.data_science import UniversalDataLoader, TextCleaner, SentimentAnalyzer
```

### 2. **×× ×”××‘×—×Ÿ ×“×•×¨×© ×¡×¤×¨×™×™×” ×¡×¤×¦×™×¤×™×ª:**
```python
analyzer = ProcessorFactory.create_sentiment_analyzer("textblob")  # ××• vader, spacy
```

### 3. **×× ×œ× ×‘×˜×•×— ××” ×–××™×Ÿ:**
```python
smart_analyzer = SmartSentimentAnalyzer()  # ×™×‘×—×¨ ×”×˜×•×‘ ×‘×™×•×ª×¨
```

### 4. **×œ×¢×‘×•×“×” ×¢× ×¢×‘×¨×™×ª:**
```python
hebrew_processor = HebrewTextProcessor()
hebrew_sentiment = HebrewSentimentAnalyzer()
```

### 5. **×œ×‘×“×™×§×•×ª ××”×™×¨×•×ª:**
```python
result = quick_sentiment_check("text to analyze")
features = extract_all_features("text for complete analysis")
```

## ğŸ† ×™×ª×¨×•× ×•×ª ×œ××‘×—×Ÿ

- âœ… **×¢××™×“×•×ª**: ×¢×•×‘×“ ×¢× ×›×œ ×¡×¤×¨×™×™×” ××• ×‘×œ×¢×“×™×”
- âœ… **×’××™×©×•×ª**: ×ª×•××š ×‘-VADER, TextBlob, spaCy, Hebrew
- âœ… **×¤×©×˜×•×ª**: ×¤×•× ×§×¦×™×•×ª one-liner ×œ××‘×—×Ÿ ××”×™×¨
- âœ… **××ª×§×“×**: Stemming, Lemmatization, Ensemble
- âœ… **×¢×‘×¨×™×ª**: ×ª××™×›×” ××œ××” ×‘×¢×™×‘×•×“ ×¢×‘×¨×™×ª
- âœ… **Factory Pattern**: ××¨×›×™×˜×§×˜×•×¨×” ××§×¦×•×¢×™×ª
- âœ… **Fallbacks**: ×ª××™×“ ×™×© ×¤×ª×¨×•×Ÿ ×’×™×‘×•×™

**×‘×”×¦×œ×—×” ×‘××‘×—×Ÿ! ğŸ¯**