# Data Science Utilities - 注专转  转 转

注专转  拽爪注转 拽 砖砖 转 拽住 转, 注转  驻专拽 拽.

##  转 注

1. [转拽 专砖转](#转拽-专砖转)
2. [ 住驻专](#-住驻专)
3. [砖砖 住住](#砖砖-住住)
4. [专 驻专](#专-驻专)
5. [转 拽](#转-拽)
6. [驻 ](#驻-)

##  转拽 专砖转

### 专砖转 注专转
```bash
pip install pandas numpy nltk
```

### 住驻专转 驻爪转 (抓)
```bash
pip install openpyxl  # 注专 拽爪 Excel
pip install pyarrow   # 注专 拽爪 Parquet
pip install lxml      # 注专 拽爪 XML/HTML
```

### 专 专砖转
```python
#   
from shared_utilities.data_science import (
    UniversalDataLoader,
    TextCleaner, 
    TextAnalyzer,
    SentimentAnalyzer
)
```

##   住驻专

```
shared-utilities/data_science/
 __init__.py              #  驻拽爪转 专转
 data_loader.py           # 注转 拽爪  驻专
 text_cleaner.py          # 拽 拽住 转拽
 text_analyzer.py         # 转 拽住 住住
 sentiment_analyzer.py    # 转 专砖转
 EXAMPLES.py             # 转 砖砖 转
```

---

##  砖砖 住住

### 1. 注转 转 (UniversalDataLoader)

```python
# 爪专转 拽 注转 转
loader = UniversalDataLoader()

# 注转 拽爪 砖
df_csv = loader.load_data("data.csv")
df_excel = loader.load_data("data.xlsx") 
df_json = loader.load_data("data.json")
df_parquet = loader.load_data("data.parquet")

# 注 注 拽抓
info = loader.get_file_info("data.csv")
print(info)
```

**驻专 转:**
- CSV, TSV
- Excel (xlsx, xls)
- JSON, JSON Lines
- Parquet
- TXT, HTML, XML

### 2. 拽 拽住 (TextCleaner)

```python
# 爪专转 拽 拽 拽住
cleaner = TextCleaner()

# 拽 住住
clean_text = cleaner.clean_text(
    "Check this AMAZING deal!!! http://example.com @user #sale",
    remove_urls=True,
    remove_mentions=True,
    remove_hashtags=True,
    to_lowercase=True
)
# 转爪: "check this amazing deal"

# 拽 DataFrame
df_clean = cleaner.clean_dataframe(
    df, 
    ['text_column'],
    remove_punctuation=True,
    remove_extra_whitespace=True
)

# 住专转 拽住 专拽 拽爪专
df_clean = cleaner.remove_empty_texts(df_clean, ['text_column'])
df_clean = cleaner.filter_by_length(df_clean, 'text_column', min_length=3, by_chars=False)
```

### 3. 转 专砖转 (SentimentAnalyzer)

```python
# 爪专转 拽 转 专砖转
sentiment_analyzer = SentimentAnalyzer()

# 转 拽住 
score = sentiment_analyzer.get_sentiment_score("I love this product!")
label = sentiment_analyzer.get_sentiment_label("I love this product!")
detailed = sentiment_analyzer.get_detailed_scores("I love this product!")

print(f"Score: {score}, Label: {label}")
# Score: 0.6369, Label: positive

# 转 DataFrame
df_with_sentiment = sentiment_analyzer.analyze_dataframe(
    df, 
    'text_column',
    add_detailed=True
)

# 住住拽转 专砖转
scores = df_with_sentiment['sentiment_score'].tolist()
stats = sentiment_analyzer.get_sentiment_statistics(scores)
print(stats)
```

### 4. 转 拽住 (TextAnalyzer)

```python
# 爪专转 拽 转 拽住
analyzer = TextAnalyzer()

# 转驻转 拽住 驻 拽专转
distribution = analyzer.analyze_text_distribution(df, 'text_column', 'category_column')

# 转 专 拽住
lengths = analyzer.analyze_text_lengths(df, 'text_column')
print(f"Average words: {lengths['average_word_count']}")

#  驻爪转
common_words = analyzer.find_common_words(df, 'text_column', top_n=10)
for word_info in common_words[:5]:
    print(f"{word_info['word']}: {word_info['count']} times")

# 驻砖 转 驻转
keywords = ['good', 'bad', 'excellent']
keyword_results = analyzer.keyword_analysis(df, 'text_column', keywords)

#  拽祝
report = analyzer.generate_summary_report(df, 'text_column', 'category_column', keywords)
```

---

##  专 驻专

### Pipeline  转 拽住

```python
def complete_text_analysis_pipeline(file_path, text_col, category_col=None):
    """Pipeline  转 拽住"""
    
    # 1. 注转 转
    loader = UniversalDataLoader()
    df = loader.load_data(file_path)
    print(f"Loaded: {len(df)} rows")
    
    # 2. 拽 拽住
    cleaner = TextCleaner()
    df_clean = cleaner.clean_dataframe(
        df, [text_col],
        remove_urls=True,
        remove_punctuation=True,
        to_lowercase=True,
        remove_extra_whitespace=True
    )
    
    # 住专转 拽住 专拽 拽爪专
    df_clean = cleaner.remove_empty_texts(df_clean, [text_col])
    df_clean = cleaner.filter_by_length(df_clean, text_col, min_length=3, by_chars=False)
    print(f"After cleaning: {len(df_clean)} rows")
    
    # 3. 转 专砖转
    sentiment_analyzer = SentimentAnalyzer()
    df_analyzed = sentiment_analyzer.analyze_dataframe(df_clean, text_col, add_detailed=True)
    
    # 4. 转 拽住
    analyzer = TextAnalyzer()
    
    #  拽祝
    keywords = ['good', 'great', 'bad', 'terrible', 'excellent']
    report = analyzer.generate_summary_report(df_analyzed, text_col, category_col, keywords)
    
    return df_analyzed, report

# 砖砖
df_result, analysis_report = complete_text_analysis_pipeline(
    "reviews.csv", 
    "review_text", 
    "product_category"
)
```

### 转 砖转  拽专转

```python
def compare_categories_sentiment(df, text_col, category_col):
    """砖转 专砖转  拽专转"""
    
    sentiment_analyzer = SentimentAnalyzer()
    df_with_sentiment = sentiment_analyzer.analyze_dataframe(df, text_col)
    
    # 住住拽转 驻 拽专
    category_stats = {}
    for category in df[category_col].unique():
        category_data = df_with_sentiment[df_with_sentiment[category_col] == category]
        scores = category_data['sentiment_score'].tolist()
        stats = sentiment_analyzer.get_sentiment_statistics(scores)
        category_stats[category] = stats
    
    return category_stats

# 砖砖
stats_by_category = compare_categories_sentiment(df, 'review_text', 'product_type')
for category, stats in stats_by_category.items():
    print(f"{category}: Avg sentiment = {stats['average_score']}")
```

---

##  转 拽 注砖转

###  1: 转 拽专转 爪专

```python
import pandas as pd
from shared_utilities.data_science import *

# 爪专转 转 
reviews_data = pd.DataFrame({
    'review_text': [
        "This product is absolutely amazing! Best purchase ever!",
        "Terrible quality, broke after one day. Very disappointed.",
        "Good value for money, does what it promises.",
        "Outstanding customer service and fast delivery!",
        "Average product, nothing special but works fine."
    ],
    'product_category': ['electronics', 'electronics', 'books', 'electronics', 'books'],
    'rating': [5, 1, 4, 5, 3]
})

# Pipeline 
def analyze_product_reviews(df):
    # 1. 拽
    cleaner = TextCleaner()
    df_clean = cleaner.clean_dataframe(
        df, ['review_text'],
        remove_punctuation=True,
        to_lowercase=True
    )
    
    # 2. 转 专砖转
    sentiment_analyzer = SentimentAnalyzer()
    df_sentiment = sentiment_analyzer.analyze_dataframe(
        df_clean, 'review_text', add_detailed=True
    )
    
    # 3. 转 拽住
    analyzer = TextAnalyzer()
    
    #  驻爪转
    common_words = analyzer.find_common_words(df_sentiment, 'review_text', top_n=10)
    
    # 转 驻 拽专
    category_analysis = analyzer.analyze_text_distribution(
        df_sentiment, 'review_text', 'product_category'
    )
    
    # 驻砖 转 驻转
    keywords = ['amazing', 'terrible', 'good', 'bad', 'excellent']
    keyword_analysis = analyzer.keyword_analysis(df_sentiment, 'review_text', keywords)
    
    return {
        'data': df_sentiment,
        'common_words': common_words,
        'category_distribution': category_analysis,
        'keywords': keyword_analysis
    }

# 爪注 转
results = analyze_product_reviews(reviews_data)

# 爪转 转爪转
print("=== 转 拽专转 爪专 ===")
print(f"住\" 拽专转: {len(results['data'])}")
print(f"转驻转 驻 拽专: {results['category_distribution']['by_category']}")

print("\n 驻爪转:")
for word in results['common_words'][:5]:
    print(f"  {word['word']}: {word['count']} 驻注 ({word['percentage']}%)")

print("\n转 驻转:")
for keyword, stats in results['keywords']['keyword_stats'].items():
    print(f"  '{keyword}': {stats['count']} 驻注")
```

###  2: 转 住 转拽

```python
def advanced_sentiment_analysis(df, text_col):
    """转 住 转拽 注 转转"""
    
    sentiment_analyzer = SentimentAnalyzer()
    df_analyzed = sentiment_analyzer.analyze_dataframe(df, text_col, add_detailed=True)
    
    # 住住拽转 转
    scores = df_analyzed['sentiment_score'].tolist()
    stats = sentiment_analyzer.get_sentiment_statistics(scores)
    
    # 拽 拽爪转 住
    positive_texts = df_analyzed[df_analyzed['sentiment_label'] == 'positive']
    negative_texts = df_analyzed[df_analyzed['sentiment_label'] == 'negative']
    neutral_texts = df_analyzed[df_analyzed['sentiment_label'] == 'neutral']
    
    # 爪转 拽住 拽爪
    most_positive = df_analyzed.loc[df_analyzed['sentiment_score'].idxmax()]
    most_negative = df_analyzed.loc[df_analyzed['sentiment_score'].idxmin()]
    
    return {
        'statistics': stats,
        'most_positive_text': most_positive[text_col],
        'most_positive_score': most_positive['sentiment_score'],
        'most_negative_text': most_negative[text_col],
        'most_negative_score': most_negative['sentiment_score'],
        'positive_count': len(positive_texts),
        'negative_count': len(negative_texts),
        'neutral_count': len(neutral_texts)
    }

# 砖砖
sentiment_insights = advanced_sentiment_analysis(reviews_data, 'review_text')
print(f"拽住  转专: {sentiment_insights['most_positive_text']}")
print(f"爪: {sentiment_insights['most_positive_score']:.3f}")
```

###  3: 注 专 注 Pipeline Functions

```python
# 砖砖 驻拽爪转 专转 -__init__.py
from shared_utilities.data_science import (
    quick_text_analysis_pipeline,
    sentiment_analysis_pipeline,
    text_cleaning_pipeline
)

# Pipeline 专 拽抓
results = quick_text_analysis_pipeline(
    file_path="reviews.csv",
    text_column="review_text", 
    category_column="category"
)

print(f"转 拽专: {results['data_info']['original_rows']}")
print(f"转 拽: {results['data_info']['final_rows']}")

# 拽 专
df_clean = text_cleaning_pipeline(
    df, 
    ['text_column'],
    remove_punctuation=True,
    to_lowercase=True,
    remove_urls=True
)

# 转 专砖转 专
df_sentiment = sentiment_analysis_pipeline(df_clean, 'text_column')
```

---

##  驻 

### 砖 砖 砖 注转:

#### 1. **驻住 (Pandas) - 砖 **
```python
# 注转 转
df = pd.read_csv("file.csv")
df = pd.read_excel("file.xlsx")

# 拽转 转
df.info()          # 注 注 注转
df.describe()      # 住住拽转 住住转
df.head()          # 5 砖专转 专砖转
df.shape           #  转

# 注 转
df.dropna()        # 住专转 注专 住专
df.fillna(0)       #  注专 住专
df.drop_duplicates()  # 住专转 驻转

# 住 拽抓
df[df['column'] > 5]  # 住
df.groupby('category').mean()  # 拽抓 砖 爪注
df.value_counts()     # 住驻专转 注专
```

#### 2. **Text Processing - 注拽专转 砖**
```python
# 拽 拽住 住住
text = text.lower()                    # 专转 转转
text = re.sub(r'[^\w\s]', '', text)   # 住专转 住 驻住拽
text = ' '.join(text.split())         # 拽 专

# 拽转 驻爪转 
df['word_count'] = df['text'].str.split().str.len()    # 住驻专转 
df['char_count'] = df['text'].str.len()                # 住驻专转 转
df['contains_keyword'] = df['text'].str.contains('word')  # 驻砖 
```

#### 3. **Sentiment Analysis -  砖 注转**
```python
# VADER Sentiment - 注专 砖
# compound: -1 (砖 ) 注 1 ( )
# pos, neu, neg:  砖住 1

# 住 住住
if compound >= 0.05:
    sentiment = "positive"
elif compound <= -0.05:
    sentiment = "negative" 
else:
    sentiment = "neutral"
```

#### 4. **File Formats -   转 **
```python
# CSV - 驻抓 转专
df = pd.read_csv("file.csv", encoding='utf-8')

# JSON -  专
df = pd.read_json("file.json")

# Excel - 注 sheets
df = pd.read_excel("file.xlsx", sheet_name="Sheet1")

# Parquet - 拽爪 
df = pd.read_parquet("file.parquet")
```

### 转 砖转 驻砖专转 :

**砖 1: 拽 转 拽住**
```python
# 转 DataFrame 注 注转 'reviews'
# 拽 转 拽住 住专 拽专转 拽爪专转 -5 

cleaner = TextCleaner()
df_clean = cleaner.clean_dataframe(
    df, ['reviews'],
    remove_punctuation=True,
    to_lowercase=True
)
df_filtered = cleaner.filter_by_length(df_clean, 'reviews', min_length=5, by_chars=False)
```

**砖 2: 砖 住住拽转 拽住**
```python
# 砖 专 爪注 砖 拽专转 驻 拽专

analyzer = TextAnalyzer()
length_stats = analyzer.analyze_text_lengths(df, 'reviews', 'category')
print(length_stats['by_category'])
```

**砖 3: 转 专砖转**
```python
# 转 专砖转 爪 转  拽专转 转

sentiment_analyzer = SentimentAnalyzer()
df_sentiment = sentiment_analyzer.analyze_dataframe(df, 'reviews')
scores = df_sentiment['sentiment_score'].tolist()
stats = sentiment_analyzer.get_sentiment_statistics(scores)
print(f"Positive percentage: {stats['positive_percentage']}%")
```

---

## 锔 驻转专 注转 驻爪转

### 注: NLTK  爪
```python
#  NLTK  , 注专转 转注专 爪 fallback
# 拽 砖 注 注 sentiment analysis 驻砖 转专
```

### 注: 拽抓  注
```python
# 拽 转 转 驻专
loader = UniversalDataLoader()
try:
    df = loader.load_data("file.csv")
except FileNotFoundError:
    print("拽抓  爪 - 拽 转 转")
except ValueError as e:
    print(f"驻专 拽抓  转: {e}")
```

### 注: 注  拽转
```python
# 转 拽 砖注 拽转
if 'text_column' in df.columns:
    # 爪注 转 驻注
    pass
else:
    print("注  拽转")
    print("注转 转:", df.columns.tolist())
```

---

##  住 拽专转 

### 爪'拽住 转 :

- [ ] **注转 转**: 注 注 CSV, JSON, Excel
- [ ] **拽 拽住**: 住专转 驻住拽, lowercase, 拽 专
- [ ] **转 专砖转**: VADER scores, 住 positive/negative/neutral
- [ ] **住住拽转 拽住**: 住驻专转 ,  驻爪转, 专 拽住
- [ ] **注 DataFrame**: 住, 拽抓, value_counts
- [ ] **Pipeline **: 注 砖 转  砖

### 驻拽转 专转 :
```python
# 注 专
from shared_utilities.data_science import *

# 转  注 砖专转
loader = UniversalDataLoader()
df = loader.load_data("file.csv")

cleaner = TextCleaner()
df = cleaner.clean_dataframe(df, ['text'], remove_punctuation=True, to_lowercase=True)

sentiment = SentimentAnalyzer()
df = sentiment.analyze_dataframe(df, 'text', add_detailed=True)

analyzer = TextAnalyzer()
report = analyzer.generate_summary_report(df, 'text')
```

**爪 ! **