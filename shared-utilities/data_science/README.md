# Data Science Utilities - Organized Architecture ğŸš€

×¢×¨×›×ª ×›×œ×™× ××§×¦×•×¢×™×ª ×¢× **××¨×›×™×˜×§×˜×•×¨×” ×××•×¨×’× ×ª**: ×”×¤×¨×“×” ×‘×¨×•×¨×” ×‘×™×Ÿ ×‘×¡×™×¡×™ ×œ××ª×§×“×, Clients × ×¤×¨×“×™× ×-Repositories, ×ª××™×›×” ××œ××” ×‘-Sync/Async.

## ğŸ“ ××‘× ×” ×××•×¨×’×Ÿ

```
shared-utilities/data_science/
â”œâ”€â”€ basic/                              # ×¨×›×™×‘×™× ×‘×¡×™×¡×™×™× (×œ×œ× ×ª×œ×•×ª×™×•×ª)
â”‚   â”œâ”€â”€ clients/                        # Connection Management
â”‚   â”‚   â”œâ”€â”€ data_loader_client.py       # ×˜×¢×™× ×ª ×§×‘×¦×™×
â”‚   â”‚   â””â”€â”€ text_processor_client.py    # ×¢×™×‘×•×“ ×˜×§×¡×˜ ×‘×¡×™×¡×™
â”‚   â””â”€â”€ repositories/                   # CRUD Operations
â”‚       â”œâ”€â”€ text_cleaning_repo.py       # × ×™×§×•×™ ×˜×§×¡×˜
â”‚       â”œâ”€â”€ text_analysis_repo.py       # × ×™×ª×•×— ×˜×§×¡×˜
â”‚       â””â”€â”€ sentiment_analysis_repo.py  # × ×™×ª×•×— ×¨×’×©×•×ª ×‘×¡×™×¡×™
â”‚
â”œâ”€â”€ advanced/                           # ×¨×›×™×‘×™× ××ª×§×“××™× (×¡×¤×¨×™×•×ª ×—×™×¦×•× ×™×•×ª)
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â”œâ”€â”€ sync/                       # ×œ×§×•×—×•×ª ×¡×™× ×›×¨×•× ×™×™×
â”‚   â”‚   â”‚   â”œâ”€â”€ sentiment_client.py     # VADER, TextBlob, spaCy
â”‚   â”‚   â”‚   â””â”€â”€ nlp_client.py          # NLTK, spaCy stemming
â”‚   â”‚   â””â”€â”€ async/                      # ×œ×§×•×—×•×ª ××¡×™× ×›×¨×•× ×™×™×
â”‚   â”‚       â”œâ”€â”€ sentiment_client.py     # Async sentiment
â”‚   â”‚       â””â”€â”€ nlp_client.py          # Async NLP
â”‚   â”‚
â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”œâ”€â”€ sync/                       # Repositories ×¡×™× ×›×¨×•× ×™×™×
â”‚   â”‚   â”‚   â”œâ”€â”€ sentiment_repo.py       # Multi-library sentiment
â”‚   â”‚   â”‚   â”œâ”€â”€ nlp_repo.py            # Stemming/Lemmatization
â”‚   â”‚   â”‚   â””â”€â”€ hebrew_repo.py         # ×¢×™×‘×•×“ ×¢×‘×¨×™×ª
â”‚   â”‚   â””â”€â”€ async/                      # Repositories ××¡×™× ×›×¨×•× ×™×™×
â”‚   â”‚       â”œâ”€â”€ sentiment_repo.py       # Async sentiment ops
â”‚   â”‚       â”œâ”€â”€ nlp_repo.py            # Async NLP ops
â”‚   â”‚       â””â”€â”€ hebrew_repo.py         # Async Hebrew ops
â”‚   â”‚
â”‚   â””â”€â”€ base/                          # Abstract Base Classes
â”‚       â”œâ”€â”€ sentiment_base.py          # SentimentAnalyzerBase
â”‚       â”œâ”€â”€ text_processor_base.py     # TextProcessorBase
â”‚       â””â”€â”€ factory.py                 # ProcessorFactory
â”‚
â””â”€â”€ utils/                              # ×›×œ×™ ×¢×–×¨ ××©×•×ª×¤×™×
    â”œâ”€â”€ config.py                       # ProcessingConfig
    â”œâ”€â”€ helpers.py                      # Pipeline functions
    â””â”€â”€ constants.py                    # ×§×‘×•×¢×™× ××©×•×ª×¤×™×
```

## ğŸ¯ ×™×ª×¨×•× ×•×ª ×”××¨×›×™×˜×§×˜×•×¨×”

### âœ… ×”×¤×¨×“×” ×‘×¨×•×¨×”
- **Basic vs Advanced**: ×¨×›×™×‘×™× ×‘×¡×™×¡×™×™× ×œ×œ× ×ª×œ×•×ª×™×•×ª, ××ª×§×“××™× ×¢× ×¡×¤×¨×™×•×ª
- **Clients vs Repositories**: × ×™×”×•×œ ×—×™×‘×•×¨×™× × ×¤×¨×“ ××¤×¢×•×œ×•×ª CRUD
- **Sync vs Async**: ×ª××™×›×” ××œ××” ×‘×©× ×™ ××•×“×œ×™×

### âœ… ×¢××™×“×•×ª ×‘××‘×—×Ÿ
- ×¨×›×™×‘×™× ×‘×¡×™×¡×™×™× **×ª××™×“ ×¢×•×‘×“×™×**
- Fallback ××•×˜×•××˜×™ ×× ×¡×¤×¨×™×•×ª ×—×¡×¨×•×ª
- ××‘× ×” ××—×™×“ ×•×¢×§×‘×™

### âœ… ×”×¨×—×‘×” ×§×œ×”
- ×›×œ ×¨×›×™×‘ ×‘××§×•× ×”× ×›×•×Ÿ ×©×œ×•
- ×”×•×¡×¤×ª ×¡×¤×¨×™×•×ª ×—×“×©×•×ª ×¤×©×•×˜×”
- ××¨×›×™×˜×§×˜×•×¨×” ××•×“×•×œ×¨×™×ª

## ğŸš€ ×©×™××•×© ××”×™×¨

### Basic Pipeline (×ª××™×“ ×¢×•×‘×“)
```python
from shared_utilities.data_science import (
    DataLoaderClient, TextCleaningRepository, 
    SentimentAnalysisRepository, create_basic_pipeline
)

# ×™×¦×™×¨×ª pipeline ×‘×¡×™×¡×™
pipeline = create_basic_pipeline(
    data_source="reviews.csv",
    text_column="review_text"
)

# ×˜×¢×™× ×ª × ×ª×•× ×™×
data_client = pipeline['components']['data_client']
with data_client.connect("reviews.csv") as conn:
    df = conn.load_data()

# × ×™×§×•×™ ×˜×§×¡×˜
cleaning_repo = pipeline['components']['cleaning_repo']
cleaning_result = cleaning_repo.create_cleaning_operation(
    data=df,
    text_columns=['review_text']
)

# × ×™×ª×•×— ×¨×’×©×•×ª
sentiment_repo = pipeline['components']['sentiment_repo']
sentiment_result = sentiment_repo.create_sentiment_analysis(
    data=cleaning_result['cleaned_data'],
    text_column='review_text'
)

print(f"Processed {len(sentiment_result['analyzed_data'])} reviews")
```

### Advanced Pipeline (×× ×–××™×Ÿ)
```python
from shared_utilities.data_science import (
    SentimentClient, NLPClient, 
    AdvancedSentimentRepository, create_advanced_pipeline
)

# ×™×¦×™×¨×ª pipeline ××ª×§×“×
pipeline = create_advanced_pipeline(
    data_source="reviews.csv",
    text_column="review_text",
    mode='sync'  # ××• 'async'
)

# × ×™×ª×•×— ×¨×’×©×•×ª ××ª×§×“×
sentiment_client = pipeline['components']['sentiment_client']
with sentiment_client.create_session() as session:
    # ×”×©×•×•××ª analyzers
    result_vader = session.analyze_with_analyzer(text, 'vader')
    result_textblob = session.analyze_with_analyzer(text, 'textblob')
    result_ensemble = session.analyze_with_ensemble(['vader', 'textblob'])

# ×¢×™×‘×•×“ NLP ××ª×§×“×
nlp_client = pipeline['components']['nlp_client']
with nlp_client.create_session() as session:
    stems = session.extract_stems(text)
    lemmas = session.extract_lemmas(text)
    roots = session.extract_roots(text)
```

### Pipeline ××•×˜×•××˜×™ (×‘×—×™×¨×” ×—×›××”)
```python
from shared_utilities.data_science import execute_complete_pipeline

# Pipeline ×©×‘×•×—×¨ ××•×˜×•××˜×™×ª ××ª ×”×˜×•×‘ ×‘×™×•×ª×¨ ×”×–××™×Ÿ
results = execute_complete_pipeline(
    data_source="reviews.csv",
    text_column="review_text",
    pipeline_type='auto',  # ×‘×•×—×¨ ××•×˜×•××˜×™×ª
    category_column="product_type"
)

print(f"Pipeline type used: {results['pipeline_info']['type']}")
print(f"Processing successful: {results['summary']['processing_successful']}")
```

## ğŸ”§ ×ª×›×•× ×•×ª ××ª×§×“××•×ª

### Factory Pattern
```python
from shared_utilities.data_science.advanced.base import ProcessorFactory

# ×™×¦×™×¨×ª analyzers ×¡×¤×¦×™×¤×™×™×
vader_analyzer = ProcessorFactory.create_sentiment_analyzer("vader")
textblob_analyzer = ProcessorFactory.create_sentiment_analyzer("textblob")

# ×™×¦×™×¨×ª processors
nltk_processor = ProcessorFactory.create_text_processor("nltk")
spacy_processor = ProcessorFactory.create_text_processor("spacy")
```

### Async Operations
```python
from shared_utilities.data_science.advanced.clients.async_ import AsyncSentimentClient

async def analyze_large_dataset():
    async_client = AsyncSentimentClient()
    
    async with async_client.create_session() as session:
        tasks = [
            session.analyze_batch_async(batch) 
            for batch in text_batches
        ]
        results = await asyncio.gather(*tasks)
    
    return results
```

### Hebrew Processing
```python
from shared_utilities.data_science.advanced.repositories.sync import HebrewRepository

hebrew_repo = HebrewRepository()

hebrew_result = hebrew_repo.create_hebrew_analysis(
    data=pd.DataFrame([{'text': '×”××•×¦×¨ ×”×–×” ××¢×•×œ×” ×•××•××œ×¥ ×‘×—×•×'}]),
    text_column='text'
)

print(f"Hebrew sentiment: {hebrew_result['results'][0]['sentiment']}")
print(f"Hebrew stems: {hebrew_result['results'][0]['stems']}")
```

## ğŸ“ ×ª×¨×—×™×©×™ ××‘×—×Ÿ

### ×ª×¨×—×™×© 1: "×”×©×ª××© ×‘-TextBlob"
```python
# ×× TextBlob ×–××™×Ÿ
try:
    analyzer = ProcessorFactory.create_sentiment_analyzer("textblob")
    result = analyzer.analyze_sentiment(text)
except:
    # Fallback ××•×˜×•××˜×™
    result = quick_sentiment_check(text, method="auto")
```

### ×ª×¨×—×™×© 2: "×‘×¦×¢ Stemming ×•-Lemmatization"
```python
# ×¢× NLTK
from shared_utilities.data_science.advanced.clients.sync import NLPClient

nlp_client = NLPClient()
with nlp_client.create_session() as session:
    stems = session.extract_stems_with_nltk(text)
    lemmas = session.extract_lemmas_with_nltk(text)

# ×¢× spaCy (×× ×–××™×Ÿ)
spacy_stems = session.extract_stems_with_spacy(text)
spacy_lemmas = session.extract_lemmas_with_spacy(text)
```

### ×ª×¨×—×™×© 3: "× ×™×ª×•×— ×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª"
```python
from shared_utilities.data_science.advanced.repositories.sync import HebrewRepository

hebrew_repo = HebrewRepository()
result = hebrew_repo.create_hebrew_analysis(
    data=pd.DataFrame([{'text': '×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª'}]),
    text_column='text'
)
```

## ğŸ“Š ×‘×“×™×§×ª ××¦×‘ ×”××¢×¨×›×ª

```python
from shared_utilities.data_science import get_system_capabilities, validate_data_science_setup

# ×‘×“×™×§×ª ×™×›×•×œ×•×ª ×”××¢×¨×›×ª
capabilities = get_system_capabilities()
print(f"Basic components: {capabilities['basic_components']}")
print(f"Advanced sync: {capabilities['advanced_sync']}")
print(f"Advanced async: {capabilities['advanced_async']}")
print(f"External libraries: {capabilities['external_libraries']}")

# ×•×œ×™×“×¦×™×” ××œ××”
validation = validate_data_science_setup()
print(f"Overall status: {validation['overall_status']}")
print(f"Recommendations: {validation['recommendations']}")
```

## ğŸ› ï¸ ×”×ª×§× ×”

### ×‘×¡×™×¡×™ (×ª××™×“ ×¢×•×‘×“)
```bash
pip install pandas numpy
```

### ××ª×§×“× (×œ×ª×›×•× ×•×ª ××œ××•×ª)
```bash
pip install pandas numpy nltk textblob spacy openpyxl
python -m spacy download en_core_web_sm
```

## ğŸ¯ ×¢×™×§×¨×™ ×”×œ×§×—×™× ×œ××‘×—×Ÿ

- âœ… **××‘× ×” ×××•×¨×’×Ÿ**: ×”×¤×¨×“×” ×‘×¨×•×¨×” ×‘×™×Ÿ ×¨×›×™×‘×™×
- âœ… **Client/Repository Pattern**: ××¨×›×™×˜×§×˜×•×¨×” ××§×¦×•×¢×™×ª
- âœ… **Sync/Async Support**: ×ª××™×›×” ×‘×©× ×™ ××•×“×œ×™×
- âœ… **×ª××™×“ ×¢×•×‘×“**: ×¨×›×™×‘×™× ×‘×¡×™×¡×™×™× ×œ×œ× ×ª×œ×•×ª×™×•×ª
- âœ… **Fallback ××•×˜×•××˜×™**: ×¢××™×“×•×ª ×‘×¤× ×™ ×¡×¤×¨×™×•×ª ×—×¡×¨×•×ª
- âœ… **Factory Pattern**: ×™×¦×™×¨×ª objects ×’××™×©×”
- âœ… **×ª××™×›×” ×‘×¢×‘×¨×™×ª**: ×¢×™×‘×•×“ ×˜×§×¡×˜ ×¢×‘×¨×™ ××œ×

**×‘×”×¦×œ×—×” ×‘××‘×—×Ÿ! ğŸ‰**